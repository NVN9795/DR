{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R0Pcwi5EDvh",
        "outputId": "ffb41236-6953-477e-9f78-cb0313f564d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 18 05:16:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    50W / 400W |    693MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v1r_qKK9E5G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu1jM81lvHR8",
        "outputId": "a3deeb6b-a0f2-466f-8d0f-ef55f1808481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGZ9GYQvIAFE"
      },
      "outputs": [],
      "source": [
        "import os,cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "\n",
        "#from keras.optimizers import gradient_descent_v2\n",
        "#sgd = gradient_descent_v2.SGD()\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg6k58HvNS9b"
      },
      "outputs": [],
      "source": [
        "pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdBrXi--NS9g"
      },
      "outputs": [],
      "source": [
        "pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjYT42uTEM7e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import os,cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "\n",
        "#from keras.optimizers import gradient_descent_v2\n",
        "#sgd = gradient_descent_v2.SGD()\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "#model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "RANDOM_SEED=42\n",
        "#Importing libraries\n",
        "import sys\n",
        "import numpy\n",
        "import xlrd #excel read\n",
        "import os\n",
        "import io\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "#from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy import signal\n",
        "from scipy.spatial import distance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow import keras, convert_to_tensor, float32, make_ndarray\n",
        "from tensorflow.compat.v1 import Session\n",
        "from tensorflow.keras import layers, backend\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, GlobalAveragePooling1D, Dropout\n",
        "from tensorflow.keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, MaxPool1D, TimeDistributed, RepeatVector, Conv2D ,AveragePooling1D\n",
        "from keras.layers import concatenate\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "import random\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, MaxPool1D, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU47fYzpNS9n"
      },
      "outputs": [],
      "source": [
        "pip install Augmentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StTA9sNQNS9o"
      },
      "outputs": [],
      "source": [
        "import Augmentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0VzD7xJNS9o"
      },
      "outputs": [],
      "source": [
        "p = Augmentor.Pipeline(\"/content/drive/MyDrive/DR_Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsrJgSSBNS9p"
      },
      "outputs": [],
      "source": [
        "p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKaCMST8NS9p"
      },
      "outputs": [],
      "source": [
        "p.sample(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBfr_PWDXo6u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dth1qrcNS9p"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "data_dir=pathlib.Path('/content/drive/MyDrive/DR_Dataset/output')\n",
        "img_height = 224\n",
        "img_width = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcKOi4qeIBEx"
      },
      "outputs": [],
      "source": [
        "PATH = os.getcwd()\n",
        "# Define data path\n",
        "data_path = '/content/drive/MyDrive/DR_Dataset/output'\n",
        "data_dir_list = os.listdir(data_path)\n",
        "\n",
        "img_rows=32\n",
        "img_cols=32\n",
        "num_channel=3\n",
        "num_epoch=22\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = 5\n",
        "\n",
        "\n",
        "\n",
        "labels_name  = {'No_DR':0, 'Mild':1, 'Moderate':2, 'Severe':3, 'Proliferative_DR':4}\n",
        "\n",
        "img_data_list=[]\n",
        "labels_list = []\n",
        "\n",
        "for dataset in data_dir_list:\n",
        "\timg_list=os.listdir(data_path+'/'+ dataset)\n",
        "\tprint ('Loading the images of dataset-'+'{}\\n'.format(dataset))\n",
        "\tlabel = labels_name[dataset]\n",
        "\tfor img in img_list:\n",
        "\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
        "\t\t#input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
        "\t\tinput_img_resize=cv2.resize(input_img,(128,128))\n",
        "\t\timg_data_list.append(input_img_resize)\n",
        "\t\tlabels_list.append(label)\n",
        "\n",
        "img_data = np.array(img_data_list)\n",
        "img_data = img_data.astype('float32')\n",
        "img_data /= 255\n",
        "print (img_data.shape)\n",
        "\n",
        "labels = np.array(labels_list)\n",
        "print(labels)\n",
        "print(num_classes)\n",
        "# print the count of number of samples for different classes\n",
        "print(np.unique(labels,return_counts=True))\n",
        "# convert class labels to on-hot encoding\n",
        "Y = np_utils.to_categorical(labels, num_classes)\n",
        "\n",
        "#Shuffle the dataset\n",
        "x,y = shuffle(img_data,Y, random_state=30)\n",
        "print(x.shape)\n",
        "print(x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw-deMSnBIqa"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=6)\n",
        "y_train=np.argmax(y_train, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "#img_data= np.expand_dims(img_data, axis=3)\n",
        "#x_train= np.expand_dims(X_train, axis=3)\n",
        "#x_test= np.expand_dims(X_test, axis=3)\n",
        "#y_train= np.expand_dims(y_train, axis=3)\n",
        "#y_test= np.expand_dims(y_test, axis=3)\n",
        "print (img_data.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Defining the model\n",
        "input_shape=img_data[0].shape\n",
        "print(\"***************************\")\n",
        "print(input_shape)\n",
        "print(\"***************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAEajqTtFc3d"
      },
      "outputs": [],
      "source": [
        "#img_data = np.expand_dims(img_data_list, axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLuILE7s9bzp"
      },
      "outputs": [],
      "source": [
        "input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B49jtaeNMhMW"
      },
      "outputs": [],
      "source": [
        "x_train= np.expand_dims(X_train, axis=3)\n",
        "x_test= np.expand_dims(X_test, axis=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4EWB4r-FXmh"
      },
      "outputs": [],
      "source": [
        "#img_data = np.expand_dims(img_data_list, axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cztEAvI3BMSy"
      },
      "outputs": [],
      "source": [
        "print(f\"x_train shape: {X_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {X_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rBdScZ6X4W5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sYxrHdbX8Kc"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "image_size = 128  # We'll resize input images to this size\n",
        "patch_size = 8  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuKXxVSHX-W0"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42hBF7PhYBZY"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEVrQ3Z2YFiC"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqW1XukoYHmB"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsTewqfcYKdh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = X_train[np.random.choice(range(X_train.shape[0]))]\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy())\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbkyeFFWYOpV"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkKWX2XcYSl2"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Dense, concatenate\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoWb88HVYUxi"
      },
      "outputs": [],
      "source": [
        "def create_convolution_layers1(x):\n",
        "\n",
        "  #inputs = keras.Input(shape=input_shape)\n",
        "  #x = inputs\n",
        "  conv1 = keras.layers.Conv2D(filters=32,activation='relu', kernel_size=3, padding=\"same\")(x)\n",
        "  conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "  conv1 = keras.layers.Conv2D(filters=64,activation='relu', kernel_size=3, padding=\"same\")(conv1)\n",
        "  conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "  #conv1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(conv1)\n",
        "  #conv1(conv1)\n",
        "  conv1 = keras.layers.Dropout(0.5)(conv1)\n",
        "\n",
        "  conv1 = keras.layers.Dropout(0.5)(conv1)\n",
        "\n",
        "  conv1 = keras.layers.Conv2D(filters=32,activation='relu', kernel_size=3, padding=\"same\")(conv1)\n",
        "  conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "  #conv1 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(conv1)\n",
        "  conv1 = keras.layers.Dropout(0.5)(conv1)\n",
        "\n",
        "  conv1 = keras.layers.Conv2D(filters=32,activation='relu', kernel_size=3, padding=\"same\")(conv1)\n",
        "  conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "  #conv1 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(conv1)\n",
        "  conv1 = keras.layers.Dropout(0.5)(conv1)\n",
        "  return conv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU4I1FIlFJbr"
      },
      "outputs": [],
      "source": [
        "#img_data = np.expand_dims(img_data_list, axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7alG-X09JqNW"
      },
      "outputs": [],
      "source": [
        "from keras.layers import concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBK8pPQwYX3V"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    #print(inputs.shape)\n",
        "    #inputs = Input(shape=input_tensor.shape[1:])\n",
        "    #x = Resizing(224, 224)(inputs)\n",
        "    #inputs=create_convolution_layers1(inputs)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    augmented1=create_convolution_layers1(augmented) # ADDITION OF CNN LAYER IN VISION TRANSFORMER\n",
        "    augmented2=create_convolution_layers1(augmented)\n",
        "    x= concatenate([  augmented1, augmented2])\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    features = layers.Flatten()(features) # Add Flatten layer\n",
        "    print(features.shape)\n",
        "    #features=create_convolution_layers1(features)\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "    #return Model(inputs=inputs, outputs=outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijdvENXsYbuF"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=X_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=50,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8dRO0YDELMK"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,y_train, batch_size= 64, epochs = 150,validation_split=0.2 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPGe9EAhELPP"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbKQWLf0E2Gq"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07cElqFNE2Jk"
      },
      "outputs": [],
      "source": [
        "y_pred1=np.argmax(y_pred,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isQ0vSx8E2Ma"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,precision_score,f1_score, precision_recall_fscore_support\n",
        "print('accuracy:', accuracy_score(b, y_pred1))\n",
        "confusion_matrix(b,y_pred1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azN87YR1E2Oz"
      },
      "outputs": [],
      "source": [
        "print(precision_recall_fscore_support(b, y_pred1, average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONEUGKuEE2Rz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train =history_T.history['loss']\n",
        "val_train =history_T.history['val_loss']\n",
        "epochs=range(0,150)\n",
        "plt.plot(epochs,loss_train,'g',label='training loss')\n",
        "plt.plot(epochs,val_train,'r',label='Val_loss')\n",
        "plt.title('training and validation loss of the proposed model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss')\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCGObMs8E2UF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc_train =history_T.history['accuracy']\n",
        "val_train =history_T.history['val_accuracy']\n",
        "epochs=range(0,150)\n",
        "plt.plot(epochs,acc_train,'g',label='training accuracy')\n",
        "plt.plot(epochs,val_train,'r',label='Val_accuracy')\n",
        "plt.title('training and validation accuracy of the proposed model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('accurary')\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gG8Oe3UbE2Xc"
      },
      "outputs": [],
      "source": [
        "matrix = metrics.confusion_matrix(b, y_pred1)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(matrix,\n",
        "            cmap=\"Blues\",\n",
        "            linecolor='black',\n",
        "            linewidths=1,\n",
        "            xticklabels=labels_name,\n",
        "            yticklabels=labels_name,\n",
        "            annot=True,\n",
        "            fmt='d')\n",
        "#cmap='PuOr',\n",
        "plt.title('Confusion Matrix ')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8R4XKK5NS-Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}